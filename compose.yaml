

networks:
  hospital-internal:
    driver: bridge
    internal: true
  hospital-proxy:
    driver: bridge
    internal: false

volumes:
  postgres_data:
  mosquitto_data:
  mosquitto_logs:
  nginx_cache:
  app_logs:
  hids_sockets:

services:
  mosquitto:
    image: eclipse-mosquitto:2-alpine
    networks: [hospital-internal]
    volumes:
      - ${RUNTIME_DIR}/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro
      - ${RUNTIME_DIR}/mosquitto/aclfile:/mosquitto/config/aclfile:ro
      - ${RUNTIME_DIR}/mosquitto/ca:/mosquitto/certs:ro
      - mosquitto_data:/mosquitto/data:rw
      - mosquitto_logs:/mosquitto/log:rw
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp:size=16m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    pids_limit: 256
    ulimits:
      nofile:
        soft: 4096
        hard: 8192
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 sh -c '</dev/tcp/127.0.0.1/8883' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s

  db:
    image: postgres:15-alpine
    networks: [hospital-internal]
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256 --auth-local=scram-sha-256
      - POSTGRES_HOST_AUTH_METHOD=scram-sha-256
    volumes:
      - postgres_data:/var/lib/postgresql/data:rw
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
      - /run/postgresql:size=16m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    pids_limit: 256
    ulimits:
      nofile:
        soft: 4096
        hard: 8192
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -h 127.0.0.1"]
      interval: 10s
      timeout: 5s
      start_period: 30s
      retries: 5

  backend:
    image: hospital-backend:${IMAGE_TAG:-latest}
    build:
      context: services/backend
      dockerfile: Dockerfile
      target: production
    networks: [hospital-internal]
    depends_on:
      db:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
    env_file:
      - ${RUNTIME_DIR}/env/backend.env
    volumes:
      - hids_sockets:/run/hids:rw
      - app_logs:/var/log/apps:rw
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    pids_limit: 256
    ulimits:
      nofile:
        soft: 8192
        hard: 16384
    user: "1001:1001"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  forwarder:
    image: hospital-forwarder:${IMAGE_TAG:-latest}
    build:
      context: services/forwarder
      dockerfile: Dockerfile
      target: production
    networks: [hospital-internal]
    depends_on:
      mosquitto:
        condition: service_healthy
      backend:
        condition: service_healthy
    env_file:
      - ${RUNTIME_DIR}/env/forwarder.env
    volumes:
      - ${RUNTIME_DIR}/mosquitto/ca:/etc/mosq-ca:ro
      - app_logs:/var/log/apps:rw
      - hids_sockets:/run/hids:rw
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp:size=16m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    pids_limit: 256
    ulimits:
      nofile:
        soft: 4096
        hard: 8192
    user: "1002:1001"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request,os; urllib.request.urlopen('http://127.0.0.1:' + os.environ.get('METRICS_PORT','9108') + '/metrics', timeout=2).read()"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 15s

  ids:
    image: hospital-ids:${IMAGE_TAG:-latest}
    build:
      context: services/ids
      dockerfile: Dockerfile
      target: production
    networks: [hospital-internal]
    depends_on:
      mosquitto:
        condition: service_healthy
      backend:
        condition: service_healthy
    env_file:
      - ${RUNTIME_DIR}/env/ids.env
    volumes:
      - ${RUNTIME_DIR}/mosquitto/ca:/etc/mosq-ca:ro
      - app_logs:/var/log/apps:rw
      - hids_sockets:/run/hids:rw
      # - /var/log/suricata:/suricata-logs:ro   # if you ingest Suricata EVE from host
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp:size=16m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    pids_limit: 256
    ulimits:
      nofile:
        soft: 4096
        hard: 8192
    user: "1003:1001"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'ids_worker.py' >/dev/null || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 15s

  frontend:
    image: hospital-frontend:${IMAGE_TAG:-latest}
    build:
      context: services/frontend
      dockerfile: Dockerfile
      target: production
    networks: [hospital-proxy]
    environment:
      - NODE_ENV=production
      - NEXT_TELEMETRY_DISABLED=1
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
      - /app/.next/cache:size=128m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    pids_limit: 256
    ulimits:
      nofile:
        soft: 4096
        hard: 8192
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  nginx:
    image: nginx:1.25-alpine
    networks: [hospital-proxy]
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ${RUNTIME_DIR}/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ${RUNTIME_DIR}/nginx/ssl:/etc/nginx/ssl:ro
      - nginx_cache:/var/cache/nginx:rw
    depends_on:
      frontend:
        condition: service_started
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
      - /run:size=16m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    cap_add: [NET_BIND_SERVICE]   # allow binding 80/443
    pids_limit: 256
    ulimits:
      nofile:
        soft: 8192
        hard: 16384
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
